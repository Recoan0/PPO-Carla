defaults:
  - _self_
  - agent: default
  - env: multi_agent
  - datasets: default
  - collector: multi_agent

wandb:
  mode: offline
  project: rl-simulator
  entity: null
  name: null
  group: iris
  tags: null
  notes: null

initialization:
  path_to_checkpoint: null
  load_actor_critic: False

common:
  epochs: 100 # TODO 100 # Used to be 600
  device: cuda:0
  do_checkpoint: True
  seed: 0
  sequence_length: ${world_model.max_blocks}
  resume: False # set by resume.sh script only.

collection:
  train:
    num_envs: 1
    stop_after_epochs: 500
    num_episodes_to_save: 10
    config:
      epsilon: 0.01
      should_sample: True
      temperature: 1.0
      num_steps: 480 # TODO 240 # 8 fps * 60s * 2min / 2 steps per action = 480 steps for 2 min of simulation
  test:
    num_envs: 1
    num_episodes_to_save: ${collection.train.num_episodes_to_save}
    config:
      epsilon: 0.0
      should_sample: True
      temperature: 0.5
      num_steps: 480
      burn_in: ${training.actor_critic.burn_in}

training:
  should: True
  learning_rate: 0.0001
  sampling_weights: [0.125, 0.125, 0.25, 0.5]
  actor_critic:
    batch_num_samples: 64
    grad_acc_steps: 1
    max_grad_norm: 10.0
    start_after_epochs: 40 # 50
    steps_per_epoch: 200
    imagine_horizon: ${common.sequence_length}
    burn_in: 20
    gamma: 0.995
    lambda_: 0.95
    entropy_weight: 0.001

evaluation:
  should: True # False # TODO True
  every: 5
  actor_critic:
    num_episodes_to_save: ${training.actor_critic.batch_num_samples}
    horizon: ${training.actor_critic.imagine_horizon}
    start_after_epochs: ${training.actor_critic.start_after_epochs}
